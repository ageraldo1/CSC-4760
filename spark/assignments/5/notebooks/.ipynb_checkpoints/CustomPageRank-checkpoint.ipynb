{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col, asc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Session\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('CustomPageRank').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load RDD\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "../resources/02AdjacencyList.txt MapPartitionsRDD[16142] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile('../resources/02AdjacencyList.txt')\n",
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vertices and Edge Dataframes\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_list = []\n",
    "e_list = []\n",
    "\n",
    "for node in rdd.map(lambda item: item.split(' ')).collect():\n",
    "    v_list.append((node[0], 'vertice_'+node[0]))\n",
    "    \n",
    "    for edge in range(1, len(node)):\n",
    "        e_list.append((node[0], node[edge]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[src: string, dst: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices = spark.createDataFrame(v_list, ['id', 'name'])\n",
    "edges = spark.createDataFrame(e_list, ['src', 'dst'])\n",
    "\n",
    "vertices.persist()\n",
    "edges.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|vertice_1|\n",
      "|  2|vertice_2|\n",
      "|  3|vertice_3|\n",
      "|  4|vertice_4|\n",
      "|  5|vertice_5|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  2|  3|\n",
      "|  2|  4|\n",
      "|  3|  4|\n",
      "|  4|  1|\n",
      "|  4|  5|\n",
      "|  5|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PageRank DataFrame\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+--------+---------+\n",
      "|page|iteration|pagerank|perc_rank|\n",
      "+----+---------+--------+---------+\n",
      "|   1|        0|     0.2|      0.2|\n",
      "|   2|        0|     0.2|      0.2|\n",
      "|   3|        0|     0.2|      0.2|\n",
      "|   4|        0|     0.2|      0.2|\n",
      "|   5|        0|     0.2|      0.2|\n",
      "+----+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l_rank = [(vertex[0], 0, 1/vertices.count(), 1/vertices.count()) for vertex in vertices.select('id').collect()]\n",
    "\n",
    "pagerank = spark.createDataFrame(l_rank, ['page', 'iteration', 'pagerank', 'perc_rank'])\n",
    "\n",
    "pagerank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform PageRank \n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_page_rank(edge, damping_factor):\n",
    "    \n",
    "    total_rank = []\n",
    "    \n",
    "    for inbound in edges.filter(f\"dst == '{edge}'\").collect():\n",
    "        last_iteration = pagerank.filter(f\"page == '{inbound['src']}'\").agg({'iteration' : 'max'}).collect()[0][0]\n",
    "        last_rank = pagerank.filter(f\"page == '{inbound['src']}'\").filter(f'iteration == {last_iteration}').select('pagerank').collect()[0][0]\n",
    "        total_outbound = edges.filter(f\"src == '{inbound['src']}'\").count()\n",
    "        \n",
    "        total_rank.append(last_rank/total_outbound)\n",
    "        \n",
    "    return (edge, last_iteration + 1, (1-damping_factor) + (damping_factor * sum(total_rank)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_factor = 0.85\n",
    "rows = []\n",
    "iterations = 29\n",
    "\n",
    "for _ in range(iterations):\n",
    "    for page in vertices.select('id').collect():\n",
    "        rows.append(calc_page_rank(page[0], damping_factor))\n",
    "  \n",
    "    newRows = spark.createDataFrame(rows)\n",
    "    sum_pr = newRows.select(newRows.columns[-1]).agg({f'{newRows.columns[-1]}' : 'sum'}).collect()[0][0]    \n",
    "    newRows = newRows.withColumn(\"perc_rank\", (newRows[f'{newRows.columns[-1]}']/sum_pr))\n",
    "    \n",
    "    pagerank = pagerank.union(newRows)\n",
    "    rows.clear() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PageRank Results\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------------+-------------------+\n",
      "|page|iteration|          pagerank|          perc_rank|\n",
      "+----+---------+------------------+-------------------+\n",
      "|   1|       29|0.7723196821160265|0.15558130445306423|\n",
      "|   2|       29|0.8056407707925519|0.16229373010028633|\n",
      "|   3|       29|1.1476149730879313|0.23118332817014126|\n",
      "|   4|       29|1.4661954349708077| 0.2953603328234441|\n",
      "|   5|       29|0.7723196821160265|0.15558130445306423|\n",
      "+----+---------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_iteration = pagerank.agg({'iteration' : 'max'}).collect()[0][0]\n",
    "\n",
    "pagerank.filter(f\"iteration == {last_iteration}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save To Disk\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank.write.format(\"csv\").save(\"../output/custom_page_rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
